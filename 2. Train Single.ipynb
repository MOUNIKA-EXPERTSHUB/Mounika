{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later.\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle\n",
    "from six.moves import range\n",
    "import scipy.io   \n",
    "import h5py\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import tarfile\n",
    "from IPython.display import display, Image\n",
    "\n",
    "from scipy import ndimage\n",
    "\n",
    "import random\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (65257, 32, 32) (65257,)\n",
      "Validation set (10000, 32, 32) (10000,)\n",
      "Test set (26032, 32, 32) (26032,)\n"
     ]
    }
   ],
   "source": [
    "pickle_file = 'SVHN_single_grey.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "    save = pickle.load(f)\n",
    "    train_dataset = save['train_dataset']\n",
    "    train_label = save['train_label']\n",
    "    valid_dataset = save['valid_dataset']\n",
    "    valid_label = save['valid_label']\n",
    "    test_dataset = save['test_dataset']f\n",
    "    test_label = save['test_label']\n",
    "    del save  # hint to help gc free up memory\n",
    "    print('Training set', train_dataset.shape, train_label.shape)\n",
    "    print('Validation set', valid_dataset.shape, valid_label.shape)\n",
    "    print('Test set', test_dataset.shape, test_label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def disp_sample_dataset(dataset, label):\n",
    "    items = random.sample(range(dataset.shape[0]), 8)\n",
    "    for i, item in enumerate(items):\n",
    "        plt.subplot(2, 4, i+1)\n",
    "        plt.axis('off')\n",
    "        plt.title(label[i])\n",
    "        plt.imshow(dataset[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Invalid dimensions for image data",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-7d7af26756b4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdisp_sample_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-9-90c996a7b980>\u001b[0m in \u001b[0;36mdisp_sample_dataset\u001b[0;34m(dataset, label)\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'off'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/RyanG/anaconda/envs/tensorflow/lib/python2.7/site-packages/matplotlib/pyplot.pyc\u001b[0m in \u001b[0;36mimshow\u001b[0;34m(X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, shape, filternorm, filterrad, imlim, resample, url, hold, data, **kwargs)\u001b[0m\n\u001b[1;32m   3027\u001b[0m                         \u001b[0mfilternorm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfilternorm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilterrad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfilterrad\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3028\u001b[0m                         \u001b[0mimlim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimlim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresample\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3029\u001b[0;31m                         **kwargs)\n\u001b[0m\u001b[1;32m   3030\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3031\u001b[0m         \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwashold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/RyanG/anaconda/envs/tensorflow/lib/python2.7/site-packages/matplotlib/__init__.pyc\u001b[0m in \u001b[0;36minner\u001b[0;34m(ax, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1817\u001b[0m                     warnings.warn(msg % (label_namer, func.__name__),\n\u001b[1;32m   1818\u001b[0m                                   RuntimeWarning, stacklevel=2)\n\u001b[0;32m-> 1819\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1820\u001b[0m         \u001b[0mpre_doc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1821\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpre_doc\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/RyanG/anaconda/envs/tensorflow/lib/python2.7/site-packages/matplotlib/axes/_axes.pyc\u001b[0m in \u001b[0;36mimshow\u001b[0;34m(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, shape, filternorm, filterrad, imlim, resample, url, **kwargs)\u001b[0m\n\u001b[1;32m   4920\u001b[0m                               resample=resample, **kwargs)\n\u001b[1;32m   4921\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4922\u001b[0;31m         \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4923\u001b[0m         \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_alpha\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4924\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_clip_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/RyanG/anaconda/envs/tensorflow/lib/python2.7/site-packages/matplotlib/image.pyc\u001b[0m in \u001b[0;36mset_data\u001b[0;34m(self, A)\u001b[0m\n\u001b[1;32m    451\u001b[0m         if (self._A.ndim not in (2, 3) or\n\u001b[1;32m    452\u001b[0m                 (self._A.ndim == 3 and self._A.shape[-1] not in (3, 4))):\n\u001b[0;32m--> 453\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid dimensions for image data\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    454\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_imcache\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Invalid dimensions for image data"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAATwAAAChCAYAAABaigMvAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n0jDwJKVh4ElK438fEx4SRrOGcQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x13702ae90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "disp_sample_dataset(train_dataset, train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (65257, 32, 32) (65257,)\n",
      "Validation set (10000, 32, 32) (10000,)\n",
      "Test set (26032, 32, 32) (26032,)\n"
     ]
    }
   ],
   "source": [
    "pickle_file = 'SVHN_single_GCN.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "    save = pickle.load(f)\n",
    "    train_dataset = save['train_dataset']\n",
    "    train_label = save['train_label']\n",
    "    valid_dataset = save['valid_dataset']\n",
    "    valid_label = save['valid_label']\n",
    "    test_dataset = save['test_dataset']\n",
    "    test_label = save['test_label']\n",
    "    del save  # hint to help gc free up memory\n",
    "    print('Training set', train_dataset.shape, train_label.shape)\n",
    "    print('Validation set', valid_dataset.shape, valid_label.shape)\n",
    "    print('Test set', test_dataset.shape, test_label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (65257, 32, 32, 1) (65257, 11)\n",
      "Validation set (10000, 32, 32, 1) (10000, 11)\n",
      "Test set (26032, 32, 32, 1) (26032, 11)\n"
     ]
    }
   ],
   "source": [
    "image_size = 32\n",
    "num_labels = 11 # 0-9, + blank \n",
    "num_channels = 1 # grayscale\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "    dataset = dataset.reshape(\n",
    "    (-1, image_size, image_size, num_channels)).astype(np.float32)\n",
    "    \n",
    "    labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "    return dataset, labels\n",
    "train_dataset, train_label = reformat(train_dataset, train_label)\n",
    "valid_dataset, valid_label = reformat(valid_dataset, valid_label)\n",
    "test_dataset, test_label = reformat(test_dataset, test_label)\n",
    "print('Training set', train_dataset.shape, train_label.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_label.shape)\n",
    "print('Test set', test_dataset.shape, test_label.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "    return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set (6000, 32, 32, 1) (6000, 11)\n"
     ]
    }
   ],
   "source": [
    "test_dataset = test_dataset[:6000,:]\n",
    "test_label = test_label[:6000,:]\n",
    "print('Test set', test_dataset.shape, test_label.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "patch_size = 5\n",
    "depth1 = 16\n",
    "depth2 = 32\n",
    "num_hidden = 1024\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "\n",
    "    def weight_varible(shape):\n",
    "        initial = tf.truncated_normal(shape, stddev = 0.1)\n",
    "        return tf.Variable(initial)\n",
    "\n",
    "    def bias_varible(shape):\n",
    "        initial = tf.constant(0.1, shape = shape)\n",
    "        return tf.Variable(initial)\n",
    "    \n",
    "    def conv2d(data, weight):\n",
    "        # strides [1, x_movement, y_movement, 1]\n",
    "        return tf.nn.conv2d(data, weight, strides = [1, 1, 1, 1], padding = 'SAME')\n",
    "\n",
    "    def max_pooling(data):\n",
    "        return tf.nn.max_pool(data, ksize = [1, 2, 2, 1], strides = [1, 2, 2, 1], padding = 'SAME')\n",
    "    \n",
    "    \n",
    "    # Input data.\n",
    "    tf_train_dataset = tf.placeholder(\n",
    "    tf.float32, shape=(batch_size, image_size, image_size, num_channels))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    \n",
    "    \n",
    "    # Varibles\n",
    "    # conv1 layer 1\n",
    "    layer1_weights = weight_varible([patch_size, patch_size, num_channels, depth1])\n",
    "    layer1_biases = bias_varible([depth1]) # 16\n",
    "    # conv2 layer 2\n",
    "    layer2_weights = weight_varible([patch_size, patch_size, depth1, depth2]) # in depth1, out depth2\n",
    "    layer2_biases = bias_varible([depth2]) # 32\n",
    "    # func1 layer 3\n",
    "    layer3_weights = weight_varible([image_size // 4 * image_size // 4 * depth2, num_hidden])\n",
    "    layer3_biases = bias_varible([num_hidden])\n",
    "    # func2 layer 4\n",
    "    layer4_weights = weight_varible([num_hidden, num_labels])\n",
    "    layer4_biases = bias_varible([num_labels])\n",
    "    \n",
    "    global_step = tf.Variable(0)  # count the number of steps taken.\n",
    "    \n",
    "    def model(dataset):\n",
    "        # conv1 layer 1\n",
    "        hidden1 = tf.nn.relu(conv2d(dataset, layer1_weights) + layer1_biases) # 32 * 32 * depth1\n",
    "        pool1 = max_pooling(hidden1) # 16 * 16 * depth1\n",
    "        # conv2 layer 2\n",
    "        hidden2 = tf.nn.relu(conv2d(pool1, layer2_weights) + layer2_biases) # 16 * 16 * depth2\n",
    "        pool2 = max_pooling(hidden2) # 8 * 8 * depth2\n",
    "        \n",
    "        shape = pool2.get_shape().as_list()\n",
    "        pool2_flat = tf.reshape(pool2, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "\n",
    "#         print([shape[0], shape[1] * shape[2] * shape[3]])\n",
    "#         print([image_size // 4 * image_size // 4 * depth2, num_hidden])\n",
    "        \n",
    "        # func1 layer 3\n",
    "        hidden3 = tf.nn.relu(tf.matmul(pool2_flat, layer3_weights) + layer3_biases)\n",
    "        hidden3_drop = tf.nn.dropout(hidden3, 0.5)\n",
    "        # func2 layer 4\n",
    "        prediction = tf.matmul(hidden3_drop, layer4_weights) + layer4_biases\n",
    "        return prediction\n",
    "    \n",
    "    \n",
    "    # Training computation.\n",
    "    logits = model(tf_train_dataset)\n",
    "    loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))\n",
    "\n",
    "    # Optimizer.\n",
    "#     learning_rate = tf.train.exponential_decay(0.5, global_step, 1000, 0.65, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.05).minimize(loss, global_step=global_step)\n",
    "\n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    valid_prediction = tf.nn.softmax(model(tf_valid_dataset))\n",
    "    test_prediction = tf.nn.softmax(model(tf_test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 15.628929\n",
      "Minibatch accuracy: 7.8%\n",
      "Validation accuracy: 15.1%\n",
      "Minibatch loss at step 500: 2.077304\n",
      "Minibatch accuracy: 28.9%\n",
      "Validation accuracy: 30.3%\n",
      "Minibatch loss at step 1000: 0.942447\n",
      "Minibatch accuracy: 69.5%\n",
      "Validation accuracy: 71.0%\n",
      "Minibatch loss at step 1500: 0.664422\n",
      "Minibatch accuracy: 82.0%\n",
      "Validation accuracy: 80.0%\n",
      "Minibatch loss at step 2000: 0.622747\n",
      "Minibatch accuracy: 78.1%\n",
      "Validation accuracy: 82.2%\n",
      "Minibatch loss at step 2500: 0.516402\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 84.1%\n",
      "Minibatch loss at step 3000: 0.246544\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 84.9%\n",
      "Minibatch loss at step 3500: 0.329750\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 85.9%\n",
      "Minibatch loss at step 4000: 0.517766\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 85.5%\n",
      "Minibatch loss at step 4500: 0.453663\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 86.0%\n",
      "Minibatch loss at step 5000: 0.410853\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 85.6%\n",
      "Minibatch loss at step 5500: 0.587102\n",
      "Minibatch accuracy: 82.0%\n",
      "Validation accuracy: 86.2%\n",
      "Minibatch loss at step 6000: 0.358548\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 87.3%\n",
      "Minibatch loss at step 6500: 0.483070\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 86.8%\n",
      "Minibatch loss at step 7000: 0.414852\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 87.5%\n",
      "Minibatch loss at step 7500: 0.385393\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 87.8%\n",
      "Minibatch loss at step 8000: 0.360902\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 88.4%\n",
      "Minibatch loss at step 8500: 0.445269\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 87.6%\n",
      "Minibatch loss at step 9000: 0.184748\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 88.1%\n",
      "Minibatch loss at step 9500: 0.303380\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 88.4%\n",
      "Minibatch loss at step 10000: 0.327126\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 88.5%\n",
      "Test accuracy: 87.4%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 10001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print('Initialized')\n",
    "    for step in range(num_steps):\n",
    "        # Pick an offset within the training data, which has been randomized.\n",
    "        # Note: we could use better randomization across epochs.\n",
    "        offset = (step * batch_size) % (train_label.shape[0] - batch_size)\n",
    "        # Generate a minibatch.\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :, :, :]\n",
    "        batch_labels = train_label[offset:(offset + batch_size), :]\n",
    "        # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "        # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "        # and the value is the numpy array to feed to it.\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "        _, l, predictions = session.run(\n",
    "          [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        if (step % 500 == 0):\n",
    "            print('Minibatch loss at step %d: %f' % (step, l))\n",
    "            print('Minibatch accuracy: %.1f%%' % accuracy(predictions, batch_labels))\n",
    "            print('Validation accuracy: %.1f%%' % accuracy(\n",
    "            valid_prediction.eval(), valid_label))\n",
    "    \n",
    "    print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## V2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "patch_size = 5\n",
    "depth1 = 16\n",
    "depth2 = 32\n",
    "depth3 = 64\n",
    "num_hidden = 256\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "\n",
    "    def weight_varible(shape):\n",
    "        initial = tf.truncated_normal(shape, stddev = 0.1)\n",
    "        return tf.Variable(initial)\n",
    "\n",
    "    def bias_varible(shape):\n",
    "        initial = tf.constant(0.1, shape = shape)\n",
    "        return tf.Variable(initial)\n",
    "    \n",
    "    def conv2d(data, weight):\n",
    "        # strides [1, x_movement, y_movement, 1]\n",
    "        return tf.nn.conv2d(data, weight, strides = [1, 1, 1, 1], padding = 'SAME')\n",
    "\n",
    "    def max_pooling(data):\n",
    "        return tf.nn.max_pool(data, ksize = [1, 2, 2, 1], strides = [1, 2, 2, 1], padding = 'SAME')\n",
    "    \n",
    "    \n",
    "    # Input data.\n",
    "    tf_train_dataset = tf.placeholder(\n",
    "    tf.float32, shape=(batch_size, image_size, image_size, num_channels))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    \n",
    "    \n",
    "    # Varibles\n",
    "    # conv1 layer 1\n",
    "    layer1_weights = weight_varible([patch_size, patch_size, num_channels, depth1])\n",
    "    layer1_biases = bias_varible([depth1]) # 16\n",
    "    # conv2 layer 2\n",
    "    layer2_weights = weight_varible([patch_size, patch_size, depth1, depth2]) # in depth1, out depth2\n",
    "    layer2_biases = bias_varible([depth2]) # 32\n",
    "    # conv3 layer 3\n",
    "    layer3_weights = weight_varible([patch_size, patch_size, depth2, depth3]) # in depth2, out depth3\n",
    "    layer3_biases = bias_varible([depth3]) # 64\n",
    "    \n",
    "    \n",
    "    # func1 layer 4\n",
    "    layer4_weights = weight_varible([image_size // 8 * image_size // 8 * depth3, num_hidden])\n",
    "    layer4_biases = bias_varible([num_hidden])\n",
    "    # func2 layer 5\n",
    "    layer5_weights = weight_varible([num_hidden, num_labels])\n",
    "    layer5_biases = bias_varible([num_labels])\n",
    "    \n",
    "    global_step = tf.Variable(0)  # count the number of steps taken.\n",
    "    \n",
    "    def model(dataset):\n",
    "        # conv1 layer 1\n",
    "        hidden1 = tf.nn.relu(conv2d(dataset, layer1_weights) + layer1_biases) # 32 * 32 * depth1\n",
    "        pool1 = max_pooling(hidden1) # 16 * 16 * depth1\n",
    "        # conv2 layer 2\n",
    "        hidden2 = tf.nn.relu(conv2d(pool1, layer2_weights) + layer2_biases) # 16 * 16 * depth2\n",
    "        pool2 = max_pooling(hidden2) # 8 * 8 * depth2\n",
    "        # conv3 layer 3\n",
    "        hidden3 = tf.nn.relu(conv2d(pool2, layer3_weights) + layer3_biases) # 8 * 8 * depth2\n",
    "        pool3 = max_pooling(hidden3) # 4 * 4 * depth3\n",
    "        \n",
    "        shape = pool3.get_shape().as_list()\n",
    "        pool3_flat = tf.reshape(pool3, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "        \n",
    "        # func1 layer 4\n",
    "        hidden4 = tf.nn.relu(tf.matmul(pool3_flat, layer4_weights) + layer4_biases)\n",
    "        hidden4_drop = tf.nn.dropout(hidden4, 0.5)\n",
    "        # func2 layer 5\n",
    "        prediction = tf.matmul(hidden4_drop, layer5_weights) + layer5_biases\n",
    "        return prediction\n",
    "    \n",
    "    \n",
    "    # Training computation.\n",
    "    logits = model(tf_train_dataset)\n",
    "    loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))\n",
    "\n",
    "    # Optimizer.\n",
    "#     learning_rate = tf.train.exponential_decay(0.5, global_step, 1000, 0.65, staircase=True)\n",
    "    optimizer = tf.train.AdamOptimizer(0.001).minimize(loss, global_step=global_step)\n",
    "\n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    valid_prediction = tf.nn.softmax(model(tf_valid_dataset))\n",
    "    test_prediction = tf.nn.softmax(model(tf_test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 9.210263\n",
      "Minibatch accuracy: 12.5%\n",
      "Validation accuracy: 15.4%\n",
      "Minibatch loss at step 500: 0.766914\n",
      "Minibatch accuracy: 76.6%\n",
      "Validation accuracy: 78.5%\n",
      "Minibatch loss at step 1000: 0.582334\n",
      "Minibatch accuracy: 79.7%\n",
      "Validation accuracy: 83.5%\n",
      "Minibatch loss at step 1500: 0.462840\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 86.5%\n",
      "Minibatch loss at step 2000: 0.463819\n",
      "Minibatch accuracy: 76.6%\n",
      "Validation accuracy: 87.1%\n",
      "Minibatch loss at step 2500: 0.411822\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 88.7%\n",
      "Minibatch loss at step 3000: 0.376943\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 89.0%\n",
      "Minibatch loss at step 3500: 0.196536\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 89.1%\n",
      "Minibatch loss at step 4000: 0.454912\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 90.3%\n",
      "Minibatch loss at step 4500: 0.316618\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 89.6%\n",
      "Minibatch loss at step 5000: 0.185370\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 90.2%\n",
      "Minibatch loss at step 5500: 0.333129\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 90.1%\n",
      "Minibatch loss at step 6000: 0.297596\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 90.2%\n",
      "Minibatch loss at step 6500: 0.553693\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 90.7%\n",
      "Minibatch loss at step 7000: 0.328277\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 90.2%\n",
      "Minibatch loss at step 7500: 0.224077\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 90.9%\n",
      "Minibatch loss at step 8000: 0.146455\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 90.2%\n",
      "Minibatch loss at step 8500: 0.104628\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 91.0%\n",
      "Minibatch loss at step 9000: 0.312537\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 90.9%\n",
      "Minibatch loss at step 9500: 0.391543\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 90.8%\n",
      "Minibatch loss at step 10000: 0.260125\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 90.4%\n",
      "Test accuracy: 89.1%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 10001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print('Initialized')\n",
    "    for step in range(num_steps):\n",
    "        # Pick an offset within the training data, which has been randomized.\n",
    "        # Note: we could use better randomization across epochs.\n",
    "        offset = (step * batch_size) % (train_label.shape[0] - batch_size)\n",
    "        # Generate a minibatch.\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :, :, :]\n",
    "        batch_labels = train_label[offset:(offset + batch_size), :]\n",
    "        # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "        # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "        # and the value is the numpy array to feed to it.\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "        _, l, predictions = session.run(\n",
    "          [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        if (step % 500 == 0):\n",
    "            print('Minibatch loss at step %d: %f' % (step, l))\n",
    "            print('Minibatch accuracy: %.1f%%' % accuracy(predictions, batch_labels))\n",
    "            print('Validation accuracy: %.1f%%' % accuracy(\n",
    "            valid_prediction.eval(), valid_label))\n",
    "    \n",
    "    print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "patch_size = 5\n",
    "depth1 = 16\n",
    "depth2 = 32\n",
    "depth3 = 64\n",
    "num_hidden = 256\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "\n",
    "    def weight_varible(shape):\n",
    "        initial = tf.truncated_normal(shape, stddev = 0.1)\n",
    "        return tf.Variable(initial)\n",
    "\n",
    "    def bias_varible(shape):\n",
    "        initial = tf.constant(0.1, shape = shape)\n",
    "        return tf.Variable(initial)\n",
    "    \n",
    "    def conv2d(data, weight):\n",
    "        # strides [1, x_movement, y_movement, 1]\n",
    "        return tf.nn.conv2d(data, weight, strides = [1, 1, 1, 1], padding = 'VALID')\n",
    "\n",
    "    def max_pooling(data):\n",
    "        return tf.nn.max_pool(data, ksize = [1, 2, 2, 1], strides = [1, 2, 2, 1], padding = 'SAME')\n",
    "    \n",
    "    \n",
    "    # Input data.\n",
    "    tf_train_dataset = tf.placeholder(\n",
    "    tf.float32, shape=(batch_size, image_size, image_size, num_channels))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    \n",
    "    \n",
    "    # Varibles\n",
    "    # conv1 layer 1\n",
    "    layer1_weights = weight_varible([patch_size, patch_size, num_channels, depth1])\n",
    "    layer1_biases = bias_varible([depth1]) # 16\n",
    "    # conv2 layer 2\n",
    "    layer2_weights = weight_varible([patch_size, patch_size, depth1, depth2]) # in depth1, out depth2\n",
    "    layer2_biases = bias_varible([depth2]) # 32\n",
    "    # conv3 layer 3\n",
    "    layer3_weights = weight_varible([patch_size, patch_size, depth2, depth3]) # in depth2, out depth3\n",
    "    layer3_biases = bias_varible([depth3]) # 64\n",
    "    \n",
    "    \n",
    "    # func1 layer 4\n",
    "    layer4_weights = weight_varible([image_size // 32 * image_size // 32 * depth3, num_hidden])\n",
    "    layer4_biases = bias_varible([num_hidden])\n",
    "    # func2 layer 5\n",
    "    layer5_weights = weight_varible([num_hidden, num_labels])\n",
    "    layer5_biases = bias_varible([num_labels])\n",
    "    \n",
    "    global_step = tf.Variable(0)  # count the number of steps taken.\n",
    "    \n",
    "    def model(dataset, keep_prob):\n",
    "        # conv1 layer 1\n",
    "        hidden1 = tf.nn.relu(conv2d(dataset, layer1_weights) + layer1_biases) # 28 * 28 * depth1\n",
    "        pool1 = max_pooling(hidden1) # 14 * 14 * depth1\n",
    "        # conv2 layer 2\n",
    "        hidden2 = tf.nn.relu(conv2d(pool1, layer2_weights) + layer2_biases) # 10 * 10 * depth2\n",
    "        pool2 = max_pooling(hidden2) # 5 * 5 * depth2\n",
    "        # conv3 layer 3\n",
    "        pool3 = tf.nn.relu(conv2d(pool2, layer3_weights) + layer3_biases) # 1 * 1 * depth3\n",
    "#         pool3 = max_pooling(hidden3) # 1 * 1 * depth3\n",
    "        \n",
    "        \n",
    "        shape = pool3.get_shape().as_list()\n",
    "        pool3_flat = tf.reshape(pool3, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "        \n",
    "        # func1 layer 4\n",
    "        hidden4 = tf.nn.relu(tf.matmul(pool3_flat, layer4_weights) + layer4_biases)\n",
    "        hidden4_drop = tf.nn.dropout(hidden4, keep_prob)\n",
    "        # func2 layer 5\n",
    "        prediction = tf.matmul(hidden4_drop, layer5_weights) + layer5_biases\n",
    "        return prediction\n",
    "    \n",
    "    \n",
    "    # Training computation.\n",
    "    logits = model(tf_train_dataset, 0.5)\n",
    "    loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))\n",
    "\n",
    "    # Optimizer.\n",
    "    learning_rate = tf.train.exponential_decay(0.001, global_step, 1000, 0.90, staircase=True)\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "\n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(model(tf_train_dataset, 1.0))\n",
    "    valid_prediction = tf.nn.softmax(model(tf_valid_dataset, 1.0))\n",
    "    test_prediction = tf.nn.softmax(model(tf_test_dataset, 1.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 3.292156\n",
      "Minibatch accuracy: 10.9%\n",
      "Validation accuracy: 11.8%\n",
      "Minibatch loss at step 500: 0.678820\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 85.4%\n",
      "Minibatch loss at step 1000: 0.627725\n",
      "Minibatch accuracy: 79.7%\n",
      "Validation accuracy: 86.7%\n",
      "Minibatch loss at step 1500: 0.406221\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 88.3%\n",
      "Minibatch loss at step 2000: 0.398645\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 89.2%\n",
      "Minibatch loss at step 2500: 0.231271\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 89.7%\n",
      "Minibatch loss at step 3000: 0.325885\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 89.8%\n",
      "Minibatch loss at step 3500: 0.197270\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 90.2%\n",
      "Minibatch loss at step 4000: 0.408744\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 90.8%\n",
      "Minibatch loss at step 4500: 0.240521\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 90.5%\n",
      "Minibatch loss at step 5000: 0.201351\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 90.5%\n",
      "Minibatch loss at step 5500: 0.388592\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 90.7%\n",
      "Minibatch loss at step 6000: 0.385200\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 91.1%\n",
      "Minibatch loss at step 6500: 0.528774\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 91.4%\n",
      "Minibatch loss at step 7000: 0.449499\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 91.3%\n",
      "Minibatch loss at step 7500: 0.179223\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 91.7%\n",
      "Minibatch loss at step 8000: 0.252187\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 90.6%\n",
      "Minibatch loss at step 8500: 0.057942\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 91.9%\n",
      "Minibatch loss at step 9000: 0.157112\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 91.7%\n",
      "Minibatch loss at step 9500: 0.442851\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 91.3%\n",
      "Minibatch loss at step 10000: 0.244647\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 91.5%\n",
      "Test accuracy: 89.1%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 10001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "#     writer = tf.train.SummaryWriter(\"logs/\", session.graph)\n",
    "    tf.initialize_all_variables().run()\n",
    "    print('Initialized')\n",
    "    for step in range(num_steps):\n",
    "        offset = (step * batch_size) % (train_label.shape[0] - batch_size)\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :, :, :]\n",
    "        batch_labels = train_label[offset:(offset + batch_size), :]\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "        _, l, predictions = session.run(\n",
    "          [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        if (step % 500 == 0):\n",
    "            print('Minibatch loss at step %d: %f' % (step, l))\n",
    "            print('Minibatch accuracy: %.1f%%' % accuracy(predictions, batch_labels))\n",
    "            print('Validation accuracy: %.1f%%' % accuracy(\n",
    "            valid_prediction.eval(), valid_label))\n",
    "    \n",
    "    print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "patch_size = 5\n",
    "depth1 = 16\n",
    "depth2 = 64\n",
    "depth3 = 128\n",
    "num_hidden = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "\n",
    "    def weight_varible(shape):\n",
    "        initial = tf.truncated_normal(shape, stddev = 0.1)\n",
    "        return tf.Variable(initial)\n",
    "\n",
    "    def bias_varible(shape):\n",
    "        initial = tf.constant(0.1, shape = shape)\n",
    "        return tf.Variable(initial)\n",
    "    \n",
    "    def conv2d(data, weight):\n",
    "        # strides [1, x_movement, y_movement, 1]\n",
    "        return tf.nn.conv2d(data, weight, strides = [1, 1, 1, 1], padding = 'SAME')\n",
    "\n",
    "    def max_pooling(data):\n",
    "        return tf.nn.max_pool(data, ksize = [1, 2, 2, 1], strides = [1, 2, 2, 1], padding = 'SAME')\n",
    "    \n",
    "    \n",
    "    # Input data.\n",
    "    tf_train_dataset = tf.placeholder(\n",
    "    tf.float32, shape=(batch_size, image_size, image_size, num_channels))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    beta_regul = tf.placeholder(tf.float32)\n",
    "    \n",
    "    \n",
    "    # Varibles\n",
    "    # conv1 layer 1\n",
    "    layer1_weights = weight_varible([patch_size, patch_size, num_channels, depth1])\n",
    "    layer1_biases = bias_varible([depth1]) # 16\n",
    "    # conv2 layer 2\n",
    "    layer2_weights = weight_varible([patch_size, patch_size, depth1, depth2]) # in depth1, out depth2\n",
    "    layer2_biases = bias_varible([depth2]) # 32\n",
    "    # conv3 layer 3\n",
    "    layer3_weights = weight_varible([patch_size, patch_size, depth2, depth3]) # in depth2, out depth3\n",
    "    layer3_biases = bias_varible([depth3]) # 64\n",
    "    \n",
    "    \n",
    "    # func1 layer 4\n",
    "    layer4_weights = weight_varible([image_size // 4 * image_size // 4 * depth3, num_hidden])\n",
    "    layer4_biases = bias_varible([num_hidden])\n",
    "    # func2 layer 5\n",
    "    layer5_weights = weight_varible([num_hidden, num_labels])\n",
    "    layer5_biases = bias_varible([num_labels])\n",
    "    \n",
    "    global_step = tf.Variable(0)  # count the number of steps taken.\n",
    "    \n",
    "    def model(dataset, keep_prob):\n",
    "        # conv1 layer 1\n",
    "        hidden1 = tf.nn.relu(conv2d(dataset, layer1_weights) + layer1_biases) # 32 * 32 * depth1\n",
    "#         hidden1 = tf.nn.local_response_normalization(hidden1)\n",
    "        pool1 = max_pooling(hidden1) # 16 * 16 * depth1\n",
    "        # conv2 layer 2\n",
    "        hidden2 = tf.nn.relu(conv2d(pool1, layer2_weights) + layer2_biases) # 16 * 16 * depth2\n",
    "#         hidden2 = tf.nn.local_response_normalization(hidden2)\n",
    "        pool2 = max_pooling(hidden2) # 8 * 8 * depth2\n",
    "        # conv3 layer 3\n",
    "        pool3 = tf.nn.relu(conv2d(pool2, layer3_weights) + layer3_biases) # 8 * 8 * depth3\n",
    "#         pool3 = max_pooling(hidden3) # 4 * 4 * depth3\n",
    "        \n",
    "        \n",
    "        shape = pool3.get_shape().as_list()\n",
    "        pool3_flat = tf.reshape(pool3, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "        \n",
    "        # func1 layer 4\n",
    "        hidden4 = tf.nn.relu(tf.matmul(pool3_flat, layer4_weights) + layer4_biases)\n",
    "        hidden4_drop = tf.nn.dropout(hidden4, keep_prob)\n",
    "        # func2 layer 5\n",
    "        prediction = tf.matmul(hidden4_drop, layer5_weights) + layer5_biases\n",
    "        return prediction\n",
    "    \n",
    "    \n",
    "    # Training computation.\n",
    "    logits = model(tf_train_dataset, 0.5)\n",
    "    loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels)) #+ beta_regul * tf.nn.l2_loss(layer5_weights)\n",
    "\n",
    "    # Optimizer.\n",
    "    learning_rate = tf.train.exponential_decay(0.001, global_step, 1000, 0.90, staircase=True)\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "\n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(model(tf_train_dataset, 1.0))\n",
    "    valid_prediction = tf.nn.softmax(model(tf_valid_dataset, 1.0))\n",
    "    test_prediction = tf.nn.softmax(model(tf_test_dataset, 1.0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 13.523302\n",
      "Minibatch accuracy: 7.8%\n",
      "Validation accuracy: 14.4%\n",
      "Minibatch loss at step 500: 2.010320\n",
      "Minibatch accuracy: 42.2%\n",
      "Validation accuracy: 46.8%\n",
      "Minibatch loss at step 1000: 1.525942\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 77.4%\n",
      "Minibatch loss at step 1500: 0.580745\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 86.4%\n",
      "Minibatch loss at step 2000: 0.595862\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 88.7%\n",
      "Minibatch loss at step 2500: 0.524747\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 89.9%\n",
      "Minibatch loss at step 3000: 0.464406\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 90.4%\n",
      "Minibatch loss at step 3500: 0.235530\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 91.2%\n",
      "Minibatch loss at step 4000: 0.479353\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 91.5%\n",
      "Minibatch loss at step 4500: 0.287734\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 91.8%\n",
      "Minibatch loss at step 5000: 0.455840\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 92.3%\n",
      "Minibatch loss at step 5500: 0.305675\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 92.3%\n",
      "Minibatch loss at step 6000: 0.207516\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 92.4%\n",
      "Minibatch loss at step 6500: 0.488661\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 92.5%\n",
      "Minibatch loss at step 7000: 0.492042\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 92.7%\n",
      "Minibatch loss at step 7500: 0.103154\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 92.9%\n",
      "Minibatch loss at step 8000: 0.167971\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 92.4%\n",
      "Minibatch loss at step 8500: 0.181505\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 92.8%\n",
      "Minibatch loss at step 9000: 0.131367\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 92.9%\n",
      "Minibatch loss at step 9500: 0.240239\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 93.0%\n",
      "Minibatch loss at step 10000: 0.202811\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 92.8%\n",
      "Test accuracy: 91.5%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 10001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print('Initialized')\n",
    "    for step in range(num_steps):\n",
    "        # Pick an offset within the training data, which has been randomized.\n",
    "        # Note: we could use better randomization across epochs.\n",
    "        offset = (step * batch_size) % (train_label.shape[0] - batch_size)\n",
    "        # Generate a minibatch.\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :, :, :]\n",
    "        batch_labels = train_label[offset:(offset + batch_size), :]\n",
    "        # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "        # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "        # and the value is the numpy array to feed to it.\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "        _, l, predictions = session.run(\n",
    "          [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        if (step % 500 == 0):\n",
    "            print('Minibatch loss at step %d: %f' % (step, l))\n",
    "            print('Minibatch accuracy: %.1f%%' % accuracy(predictions, batch_labels))\n",
    "            print('Validation accuracy: %.1f%%' % accuracy(\n",
    "            valid_prediction.eval(), valid_label))\n",
    "    \n",
    "    print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "patch_size = 5\n",
    "depth1 = 16\n",
    "depth2 = 64\n",
    "depth3 = 128\n",
    "num_hidden = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "\n",
    "    def weight_varible(shape):\n",
    "        initial = tf.truncated_normal(shape, stddev = 0.1)\n",
    "        return tf.Variable(initial)\n",
    "\n",
    "    def bias_varible(shape):\n",
    "        initial = tf.constant(0.1, shape = shape)\n",
    "        return tf.Variable(initial)\n",
    "    \n",
    "    def conv2d(data, weight):\n",
    "        # strides [1, x_movement, y_movement, 1]\n",
    "        return tf.nn.conv2d(data, weight, strides = [1, 1, 1, 1], padding = 'SAME')\n",
    "\n",
    "    def max_pooling(data):\n",
    "        return tf.nn.max_pool(data, ksize = [1, 2, 2, 1], strides = [1, 2, 2, 1], padding = 'SAME')\n",
    "    \n",
    "    \n",
    "    # Input data.\n",
    "    tf_train_dataset = tf.placeholder(\n",
    "    tf.float32, shape=(batch_size, image_size, image_size, num_channels))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    beta_regul = tf.placeholder(tf.float32)\n",
    "    \n",
    "    \n",
    "    # Varibles\n",
    "    # conv1 layer 1\n",
    "    layer1_weights = weight_varible([patch_size, patch_size, num_channels, depth1])\n",
    "    layer1_biases = bias_varible([depth1]) # 16\n",
    "    # conv2 layer 2\n",
    "    layer2_weights = weight_varible([patch_size, patch_size, depth1, depth2]) # in depth1, out depth2\n",
    "    layer2_biases = bias_varible([depth2]) # 32\n",
    "    # conv3 layer 3\n",
    "    layer3_weights = weight_varible([patch_size, patch_size, depth2, depth3]) # in depth2, out depth3\n",
    "    layer3_biases = bias_varible([depth3]) # 64\n",
    "    \n",
    "    \n",
    "    # func1 layer 4\n",
    "    layer4_weights = weight_varible([image_size // 4 * image_size // 4 * depth3, num_hidden])\n",
    "    layer4_biases = bias_varible([num_hidden])\n",
    "    # func2 layer 5\n",
    "    layer5_weights = weight_varible([num_hidden, num_labels])\n",
    "    layer5_biases = bias_varible([num_labels])\n",
    "    \n",
    "    global_step = tf.Variable(0)  # count the number of steps taken.\n",
    "    \n",
    "    def model(dataset, keep_prob):\n",
    "        # conv1 layer 1\n",
    "        hidden1 = tf.nn.relu(conv2d(dataset, layer1_weights) + layer1_biases) # 32 * 32 * depth1\n",
    "#         hidden1 = tf.nn.local_response_normalization(hidden1)\n",
    "        pool1 = max_pooling(hidden1) # 16 * 16 * depth1\n",
    "        # conv2 layer 2\n",
    "        hidden2 = tf.nn.relu(conv2d(pool1, layer2_weights) + layer2_biases) # 16 * 16 * depth2\n",
    "#         hidden2 = tf.nn.local_response_normalization(hidden2)\n",
    "        pool2 = max_pooling(hidden2) # 8 * 8 * depth2\n",
    "        # conv3 layer 3\n",
    "        pool3 = tf.nn.relu(conv2d(pool2, layer3_weights) + layer3_biases) # 8 * 8 * depth3\n",
    "        pool3 = max_pooling(hidden3) # 4 * 4 * depth3\n",
    "        \n",
    "        \n",
    "        shape = pool3.get_shape().as_list()\n",
    "        pool3_flat = tf.reshape(pool3, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "        \n",
    "        # func1 layer 4\n",
    "        hidden4 = tf.nn.relu(tf.matmul(pool3_flat, layer4_weights) + layer4_biases)\n",
    "        hidden4_drop = tf.nn.dropout(hidden4, keep_prob)\n",
    "        # func2 layer 5\n",
    "        prediction = tf.matmul(hidden4_drop, layer5_weights) + layer5_biases\n",
    "        return prediction\n",
    "    \n",
    "    \n",
    "    # Training computation.\n",
    "    logits = model(tf_train_dataset, 0.5)\n",
    "    loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels)) + beta_regul * tf.nn.l2_loss(layer5_weights)\n",
    "\n",
    "    # Optimizer.\n",
    "    learning_rate = tf.train.exponential_decay(0.001, global_step, 1000, 0.90, staircase=True)\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "\n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(model(tf_train_dataset, 1.0))\n",
    "    valid_prediction = tf.nn.softmax(model(tf_valid_dataset, 1.0))\n",
    "    test_prediction = tf.nn.softmax(model(tf_test_dataset, 1.0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 9.831328\n",
      "Minibatch accuracy: 12.5%\n",
      "Validation accuracy: 13.2%\n",
      "Minibatch loss at step 500: 1.983049\n",
      "Minibatch accuracy: 40.6%\n",
      "Validation accuracy: 40.2%\n",
      "Minibatch loss at step 1000: 1.251227\n",
      "Minibatch accuracy: 70.3%\n",
      "Validation accuracy: 77.0%\n",
      "Minibatch loss at step 1500: 0.904425\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 86.1%\n",
      "Minibatch loss at step 2000: 0.785581\n",
      "Minibatch accuracy: 79.7%\n",
      "Validation accuracy: 88.1%\n",
      "Minibatch loss at step 2500: 0.458895\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 89.8%\n",
      "Minibatch loss at step 3000: 0.523632\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 90.3%\n",
      "Minibatch loss at step 3500: 0.376672\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 90.8%\n",
      "Minibatch loss at step 4000: 0.612246\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 91.7%\n",
      "Minibatch loss at step 4500: 0.259714\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 91.6%\n",
      "Minibatch loss at step 5000: 0.206135\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 92.3%\n",
      "Minibatch loss at step 5500: 0.419941\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 91.9%\n",
      "Minibatch loss at step 6000: 0.202911\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 92.7%\n",
      "Minibatch loss at step 6500: 0.843095\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 92.6%\n",
      "Minibatch loss at step 7000: 0.285361\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 92.9%\n",
      "Minibatch loss at step 7500: 0.130247\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 92.7%\n",
      "Minibatch loss at step 8000: 0.180052\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 92.5%\n",
      "Minibatch loss at step 8500: 0.157389\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 92.8%\n",
      "Minibatch loss at step 9000: 0.265355\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 93.2%\n",
      "Minibatch loss at step 9500: 0.492455\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 93.1%\n",
      "Minibatch loss at step 10000: 0.138511\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 93.2%\n",
      "Test accuracy: 92.1%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 10001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "\n",
    "    tf.initialize_all_variables().run()\n",
    "    print('Initialized')\n",
    "    for step in range(num_steps):\n",
    "        offset = (step * batch_size) % (train_label.shape[0] - batch_size)\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :, :, :]\n",
    "        batch_labels = train_label[offset:(offset + batch_size), :]\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, beta_regul : 1e-3}\n",
    "        _, l, predictions = session.run(\n",
    "          [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        if (step % 500 == 0):\n",
    "            print('Minibatch loss at step %d: %f' % (step, l))\n",
    "            print('Minibatch accuracy: %.1f%%' % accuracy(predictions, batch_labels))\n",
    "            print('Validation accuracy: %.1f%%' % accuracy(\n",
    "            valid_prediction.eval(), valid_label))\n",
    "    \n",
    "    print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
